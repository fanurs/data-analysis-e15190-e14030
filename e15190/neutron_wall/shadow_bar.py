import concurrent.futures
import pathlib

import numpy as np
import pandas as pd
import uproot 

from e15190 import PROJECT_DIR

class ShadowBar:
    database_dir = PROJECT_DIR / 'database/neutron_wall/shadow_bar'
    root_files_dir = PROJECT_DIR / 'database/root_files'
    light_GM_range = [5.0, 500.0] # MeVee
    pos_range = [-120.0, 120.0] # cm

    def __init__(self, AB, max_workers=8):
        """Initialize the :py:class:`ShadowBar` class.

        Parameters
        ----------
        AB : str
            Either 'A' or 'B'. To specify neutron wall A or B.
        max_workers : int, default 12
            Maximum number of workers to use for parallelization. The parallelization
            is used to read in data from ROOT files as executors for
            `decompression <https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyFile.html#decompression-executor>`__
            and
            `interpretation <https://uproot.readthedocs.io/en/latest/uproot.reading.ReadOnlyFile.html#interpretation-executor>`__
            in
            `uproot <https://uproot.readthedocs.io/>`__.
        """
        self.AB = AB.upper()
        self.ab = self.AB.lower()
        self.decompression_executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        self.interpretation_executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        self.database_dir.mkdir(parents=True, exist_ok=True)
    
    def _cut_for_root_file_data(self, AB):
        """A simple cut to select valid entries only.

        This is not the final cut. It is only the biggest cut that defines the
        largest subset of data that will be used, i.e. we only throw away
        entries that are not useable or recoverable. Data that are not of our
        *interest* should *not* be thrown away here.
        """
        AB = AB.upper()
        cuts = [
            f'NW{AB}_light_GM > {self.light_GM_range[0]}',
            f'NW{AB}_light_GM < {self.light_GM_range[1]}',
            f'NW{AB}_pos > {self.pos_range[0]}',
            f'NW{AB}_pos < {self.pos_range[1]}',
        ]
        return ' & '.join([f'({c.strip()})' for c in cuts])

    def read_run_from_root_file(self, run, tree_name=None, apply_cut=True):
        """Read in single run from ROOT file.

        The ROOT file here refers to the one generated by ``calibrate.cpp``,
        *not* the "Daniele's ROOT file".

        Parameters
        ----------
        run : int
            Run number.
        tree_name : str, default None
            Name of the tree in the ROOT file. If not specified, the function
            will try to automatically determine the tree name. If multiple trees
            are found within the ROOT file, an exception will be raised as the
            function has no way to know which tree to use.
        apply_cut : bool, default True
            If set to `True`, the function will apply the cut specified by
            :py:func:`_cut_for_root_file_data` to the data. Otherwise, the cut
            is not applied.
        
        Returns
        -------
        df : pd.DataFrame
            Dataframe containing the data for the specified run.
        """
        path = self.root_files_dir / f'run-{run:04d}.root'

        # determine the tree_name
        if tree_name is None:
            with uproot.open(str(path)) as file:
                objects = list(set(key.split(';')[0] for key in file.keys()))
            if len(objects) == 1:
                tree_name = objects[0]
            else:
                raise Exception(f'Multiple objects found in {path}')

        # load in the data
        branches = [
             'MB_multi',
            f'NW{self.AB}_bar',
            f'NW{self.AB}_light_GM',
            f'NW{self.AB}_pos',
            f'NW{self.AB}_theta',
            f'NW{self.AB}_phi',
            f'NW{self.AB}_psd',
            f'NW{self.AB}_distance',
            f'NW{self.AB}_time',
             'FA_time_min',
        ]
        with uproot.open(str(path) + ':' + tree_name) as tree:
            df = tree.arrays(
                branches,
                library='pd',
                decompression_executor=self.decompression_executor,
                interpretation_executor=self.interpretation_executor,
            )

        if apply_cut:
            df = df.query(self._cut_for_root_file_data(self.AB))

        # to-do:
        # select bars of interest only
        # calculate TOF from NW{self.AB}_time - FA_time_min
        # calculate kinetic energy from TOF and NW{self.AB}_distance
        # drop branches that won't be used,
        # e.g. NW{self.AB}_distance, NW{self.AB}_time, FA_time_min

        return df

    def cache_run(self, run, tree_name=None):
        """Read in data from ROOT file and save relevant branches to an HDF5 file.

        The data will be grouped according to bar number, because future
        retrieval by this class will most likely analyze only one bar at a time.

        Parameters
        ----------
        run : int
            Run number.
        tree_name : str, default None
            Name of the tree in the ROOT file. If not specified, the function
            will try to automatically determine the tree name. If multiple trees
            are found within the ROOT file, an exception will be raised as the
            function has no way to know which tree to use.
        """
        path = self.database_dir / f'cache/run-{run:04d}.h5'
        path.parent.mkdir(parents=True, exist_ok=True)
        df = self.read_run_from_root_file(run, tree_name=tree_name)

        # convert all float64 columns into float32
        for col in df.columns:
            if df[col].dtype == np.float64:
                df[col] = df[col].astype(np.float32)

        # write cache to HDF5 files bar by bar
        columns = [col for col in df.columns if col != f'NW{self.AB}_bar'] # drop bar number
        pathlib.Path(path).unlink(missing_ok=True) # remove pre-existing file, if any
        for bar, subdf in df.groupby(f'NW{self.AB}_bar'):
            subdf.reset_index(drop=True, inplace=True)
            with pd.HDFStore(path, mode='a') as file:
                file.put(f'nw{self.ab}{bar:02d}', subdf[columns], format='fixed')
    
    def _read_single_run(self, run, bar, from_cache=True):
        """Read in single run from either cache or ROOT file.

        If no cache is available or ``from_cache`` is set to ``False``, the
        function first prepare a cache for the run by invoking
        :py:func:`cache_run`, then read in the data from the newly created cache
        into a dataframe to be returned. Otherwise, it reads directly from the
        cache and returns the dataframe.

        Notice that the cache file is always generated for one run at a time,
        with multiple dataframes contained in it, each dataframe corresponding
        to one bar. In other words, if you have invoked
        ``_read_single_run(6666, 1)``,
        then a cache file ``cache/run-6666.h5`` will be generated, and in it
        there are dataframes for not just bar-01, but also *all* the other bars
        (of course, excluding those that you had chosen to filter out).

        Parameters
        ----------
        run : int
            Run number.
        bar : int
            Bar number.
        from_cache : bool, default True
            If set to `True`, the data will be read from the cache if exists.
            Otherwise, it will refresh the cache by reading again from the ROOT
            file, then read from the newly created cache.
        
        Returns
        -------
        df : pd.DataFrame
            Dataframe containing the data for the specified run and bar.
        """
        path = self.database_dir / f'cache/run-{run:04d}.h5'
        if not from_cache or not path.exists():
            self.cache_run(run)
        
        with pd.HDFStore(path, mode='r') as file:
            df = file.get(f'nw{self.ab}{bar:02d}')
        return df
    
    def read(self, run, bar, from_cache=True, verbose=False):
        """Read in the data needed to do shadow bar analysis.

        This function reads in the data as a dataframe, and saves it to
        ``self.df``. It also updates ``self.bar`` for future reference.

        The data will be read from the HDF5 files if they exists, otherwise the
        function will read in from the ROOT files (generated by
        ``calibrate.cpp``).

        Branch names that start with NW{self.AB}_ will be dropped because of
        redunancy. For example, ``NWB_psd`` will be reduced into ``psd``.
        However, other prefixes that indicate detectors other than the neutron
        wall will be kept, e.g. ``MB_multi``, ``FA_time_min``, etc.

        Parameters
        ----------
        run : int or list of ints
            The run number(s).
        bar : int
            Bar number.
        from_cache : bool, default True
            If set to `True`, the data will be read from the cache if exists.
            Otherwise, it will refresh the cache by reading again from the ROOT
            file, then read from the newly created cache.
        verbose : bool, default False
            Whether to print out the progress of the read in.

        Examples
        >>> from e15190.neutron_wall.shadow_bar import ShadowBar
        >>> shade = ShadowBar('B')
        >>> shade.read([4083, 4084], 1, verbose=True)
        Reading run-4083  (1/2)
        Reading run-4084  (2/2)
        >>> shade.df[['run', 'theta']]
                run      theta
        0      4083  32.787231
        1      4083  33.294674
        2      4083  35.215828
        3      4083  40.720539
        4      4083  31.086285
        ...     ...        ...
        25253  4084  38.263584
        25254  4084  35.900543
        25255  4084  40.574760
        25256  4084  31.906084
        25257  4084  49.476383

        [25258 rows x 2 columns]

        """
        if isinstance(run, int):
            runs = [run]
        else:
            runs = run
        
        df = None
        for i_run, run in enumerate(runs):
            if verbose:
                print(f'\rReading run-{run:04d}  ({i_run + 1}/{len(runs)})', end='', flush=True)
            df_run = self._read_single_run(run, bar, from_cache=from_cache)
            df_run.insert(0, 'run', run)
            if df is None:
                df = df_run.copy()
            else:
                df = pd.concat([df, df_run], ignore_index=True)
        if verbose:
            print('\r', flush=True)
        self.bar = bar
        self.df = df
        self.df.columns = [name.replace(f'NW{self.AB}_', '') for name in self.df.columns]
